# Ollama-specific settings for Containerised Deployments
#
# IMPORTANT NOTE:
# This file is primarily intended for scenarios where Ollama runs inside a Docker container.
# In the current hybrid setup (native Windows Ollama for GPU acceleration),
# these environment variables are NOT directly read or used by the running native Ollama server.
#
# When you make an API request to your native Ollama server, you specify the model
# (e.g., 'qwen3:8b') directly in the API request body.
#
# You may keep this file for reference or for future use if you were to
# revert to a fully containerised Ollama setup, or if you manually load
# these variables into your environment before running `ollama serve`.

# Example Ollama configuration variables (not active in current setup):
# MODEL_NAME=qwen3:8b
# MODEL_QUANTIZATION=q4_K_M
# OLLAMA_HOST=0.0.0.0:11434
# OLLAMA_MAX_LOADED_MODELS=1
# OLLAMA_NUM_PARALLEL=2
# OLLAMA_FLASH_ATTENTION=1
# HSA_OVERRIDE_GFX_VERSION=11.0.0
# ROCM_VERSION=5.7
# GPU_MEMORY_FRACTION=0.9
# OLLAMA_KEEP_ALIVE=24h
# OLLAMA_LOAD_TIMEOUT=300
# MAX_REQUEST_SIZE=50MB
# WORKER_PROCESSES=auto