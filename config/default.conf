# default.conf
# Nginx reverse proxy config for Ollama API
# This file is included within the http block of the main nginx.conf

# Upstream to native Windows Ollama server
upstream ollama_backend {
    server host.docker.internal:11434;
}

server {
    listen 80;
    server_name localhost wsl2-w11.jerryagenyi.xyz;

    # Root endpoint for health/info
    location / {
        return 200 "Ollama Nginx Proxy is running.";
        add_header Content-Type text/plain;
    }

    # Proxy all /api/ requests to Ollama backend
    location /api/ {
        proxy_pass http://ollama_backend/api/;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_buffering off;
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
        proxy_connect_timeout 300s;
    }
}