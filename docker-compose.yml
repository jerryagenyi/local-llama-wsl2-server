# docker-compose.yml for Llama Local Server with GPU via native Windows Ollama
services:
  nginx:
    image: nginx:alpine
    container_name: llama-nginx
    restart: unless-stopped
    ports:
      - "${EXTERNAL_NGINX_PORT}:80"
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11434
    volumes:
      - ./config:/etc/nginx/conf.d:ro
    networks:
      - llama-network

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: llama-tunnel
    restart: unless-stopped
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    # Remove the CLOUDFLARED_OPTS - use command instead
    command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TUNNEL_TOKEN}
    depends_on:
      - nginx
    networks:
      - llama-network

  watchtower:
    image: containrrr/watchtower
    container_name: llama-watchtower
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_SCHEDULE=0 0 2 * * *  # Fixed cron syntax
    networks:
      - llama-network

  n8n:
    image: n8nio/n8n:latest
    container_name: desktop-n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    environment:
      - N8N_HOST=n8n.jerryagenyi.xyz
      - N8N_PROTOCOL=https
      - WEBHOOK_URL=https://n8n.jerryagenyi.xyz/webhook/
      - GENERIC_TIMEZONE=Europe/London
      - N8N_SECURE_COOKIE=false  # Set to true if using HTTPS
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - llama-network

# Docker volumes for persistent data
volumes:
  ollama_data:
    driver: local
  n8n_data:
    driver: local

# Define custom network for containers to communicate
networks:
  llama-network:
    driver: bridge