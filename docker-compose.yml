# docker-compose.yml for Llama Local Server with GPU via native Windows Ollama and LiteLLM

services:
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    container_name: llama-litellm
    restart: unless-stopped
    environment:
      - LITELLM_PORT=4000
      - LITELLM_MODEL_LIST_FILE=/etc/litellm-config.json
      - LITELLM_LOG_LEVEL=INFO
      - LITELLM_MASTER_KEY=sk-dtOcy2cRauc03Oe8W/HpJYfQiHo8hyAKGw6o22+eLc3VbGH256cIghG4t58BYMb # Also in nginx config, so if you change here, change there too. 
    ports:
      - "4000:4000"
    networks:
      - llama-network
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --spider http://localhost:4000/models || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - ./config/litellm-config.json:/etc/litellm-config.json:ro

  nginx:
    image: nginx:alpine
    container_name: llama-nginx
    restart: unless-stopped
    ports:
      - "${EXTERNAL_NGINX_PORT}:80"
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11434
    volumes:
      - ./config:/etc/nginx/conf.d:ro
      - ./config/.htpasswd:/etc/nginx/.htpasswd:ro
    networks:
      - llama-network
    depends_on:
      - litellm
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --spider http://localhost/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: llama-tunnel
    restart: unless-stopped
    # environment:
    #   - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN} # Commented out, handled in config.yml
    command: tunnel --config /etc/cloudflared/config.yml run
    depends_on:
      - nginx
    networks:
      - llama-network
    volumes:
      - /mnt/c/Users/Username/.cloudflared:/etc/cloudflared:ro
    healthcheck:
      test: ["CMD-SHELL", "cloudflared --version || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3

  watchtower:
    image: containrrr/watchtower
    container_name: llama-watchtower
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_SCHEDULE=0 0 2 * * *  # Fixed cron syntax
    networks:
      - llama-network
    # Watchtower does not expose a health endpoint; removing healthcheck to prevent errors

  n8n:
    image: n8nio/n8n:latest
    container_name: desktop-n8n
    restart: unless-stopped
    environment:
      - N8N_HOST=n8n.jerryagenyi.xyz
      - N8N_PROTOCOL=https
      - WEBHOOK_URL=http://nginx/webhook
      - GENERIC_TIMEZONE=Europe/London
      - N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE=true
      - EXECUTIONS_PROCESS=main
      - N8N_SECURE_COOKIE=true
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - llama-network
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --spider http://localhost:5678/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    ports:
      - "127.0.0.1:5678:5678"  # temporarily expose for local access


# Docker volumes for persistent data
volumes:
  n8n_data:
    driver: local

# Define custom network for containers to communicate
networks:
  llama-network:
    driver: bridge