# docker-compose.yml for Llama Local Server with GPU via native Windows Ollama
services:
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    container_name: llama-litellm
    restart: unless-stopped
    environment:
      - LITELLM_PORT=4000
      - LITELLM_MODEL_LIST_FILE=/etc/litellm-config.json
      - LITELLM_LOG_LEVEL=INFO
    ports:
      - "4000:4000"
    networks:
      - llama-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    volumes:
      - ./config/litellm-config.json:/etc/litellm-config.json:ro

  nginx:
    image: nginx:alpine
    container_name: llama-nginx
    restart: unless-stopped
    ports:
      - "${EXTERNAL_NGINX_PORT}:80"
    environment:
      - OLLAMA_HOST=http://host.docker.internal:11434
    volumes:
      - ./config:/etc/nginx/conf.d:ro
      - ./config/.htpasswd:/etc/nginx/.htpasswd:ro
    networks:
      - llama-network
    depends_on:
      - litellm
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 5s
      retries: 3

  cloudflared:
    image: cloudflare/cloudflared:latest
    container_name: llama-tunnel
    restart: unless-stopped
    environment:
      - TUNNEL_TOKEN=${CLOUDFLARE_TUNNEL_TOKEN}
    # Remove the CLOUDFLARED_OPTS - use command instead
    command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TUNNEL_TOKEN}
    depends_on:
      - nginx
    networks:
      - llama-network
    healthcheck:
      test: ["CMD", "cloudflared", "--version"]
      interval: 60s
      timeout: 10s
      retries: 3

  watchtower:
    image: containrrr/watchtower
    container_name: llama-watchtower
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_SCHEDULE=0 0 2 * * *  # Fixed cron syntax
    networks:
      - llama-network

  n8n:
    image: n8nio/n8n:latest
    container_name: desktop-n8n
    restart: unless-stopped
    environment:
      - N8N_HOST=n8n.jerryagenyi.xyz
      - N8N_PROTOCOL=https
      - WEBHOOK_URL=http://nginx/webhook
      - GENERIC_TIMEZONE=Europe/London
      - N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE=true
      - EXECUTIONS_PROCESS=main
      - N8N_SECURE_COOKIE=true
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - llama-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5678/healthz" ]
      interval: 30s
      timeout: 10s
      retries: 3
    ports:
      - "127.0.0.1:5678:5678"  # temporarily expose for local access


# Docker volumes for persistent data
volumes:
  n8n_data:
    driver: local

# Define custom network for containers to communicate
networks:
  llama-network:
    driver: bridge