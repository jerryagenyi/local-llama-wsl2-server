# docker-compose.yml for Llama Local Server with GPU via native Windows Ollama and LiteLLM

version: '3.8'

services:
  caddy:
    image: caddy:2-alpine
    container_name: llama-caddy
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - caddy_data:/data
      - caddy_config:/config
    networks:
      - llama-network
    healthcheck:
      test: ["CMD", "caddy", "validate", "--config", "/etc/caddy/Caddyfile"]
      interval: 30s
      timeout: 10s
      retries: 3

  n8n:
    image: n8nio/n8n:latest
    container_name: desktop-n8n
    restart: unless-stopped
    environment:
      - N8N_HOST=n8n.jerryagenyi.xyz
      - N8N_PROTOCOL=https
      - WEBHOOK_URL=https://n8n.jerryagenyi.xyz/webhook
      - GENERIC_TIMEZONE=Europe/London
      - N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE=true
      - EXECUTIONS_PROCESS=main
      - N8N_SECURE_COOKIE=true
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_AUTH_USER}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_AUTH_PASSWORD}
    volumes:
      - n8n_data:/home/node/.n8n
    networks:
      - llama-network
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --spider http://localhost:5678/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  flowise:
    image: flowiseai/flowise:latest
    container_name: desktop-flowise
    restart: unless-stopped
    environment:
      - PORT=3000
      - FLOWISE_USERNAME=${FLOWISE_AUTH_USER}
      - FLOWISE_PASSWORD=${FLOWISE_AUTH_PASSWORD}
      - EXECUTION_MODE=main
      - TOOL_FUNCTION_TIMEOUT=60000
      - FLOWISE_SECRETKEY=${FLOWISE_SECRET_KEY}
    volumes:
      - flowise_data:/home/node/.flowise
    networks:
      - llama-network
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --spider http://localhost:3000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  watchtower:
    image: containrrr/watchtower
    container_name: llama-watchtower
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_SCHEDULE=0 0 2 * * *
    networks:
      - llama-network

volumes:
  n8n_data:
    driver: local
  flowise_data:
    driver: local
  caddy_data:
    driver: local
  caddy_config:
    driver: local

networks:
  llama-network:
    driver: bridge