# .env.example for Local LLM Server (Windows, WSL2, Docker, Cloudflare Tunnel)
#
# This file serves as a template for your .env file, which holds configuration
# variables for your local LLM server setup.
#
# IMPORTANT: Copy this file to a new file named `.env` in the root of your project
# and fill in the required values. Never commit your actual `.env` file to a public
# repository as it contains sensitive information.
#
# ---
# For enhanced security and organisation, consider splitting these variables
# into two separate files in a production environment:
# - `.env.public`: For non-sensitive, shared configuration.
# - `.env.secret`: For sensitive, private credentials.
# ---


# === SYSTEM & DOCKER CONFIGURATION (Non-Sensitive) ===
# These typically remain constant for the project.
COMPOSE_PROJECT_NAME=desktop-llm-server # Custom project name for Docker Compose
DOCKER_BUILDKIT=1                     # Enable Docker BuildKit for better build performance

# === NETWORKING & PROXY CONFIGURATION (Non-Sensitive) ===
# Ports and allowed origins for accessing your Nginx proxy.
# EXTERNAL_NGINX_PORT is the port Nginx will be exposed on your Windows host via Docker.
EXTERNAL_NGINX_PORT=8080
NGINX_INTERNAL_PORT=80 # Nginx listens internally on this port within its container

# Set your public domain(s) and any browser extension IDs you trust (comma-separated)
# This is crucial for CORS settings in Nginx if needed.
ALLOWED_ORIGINS=https://llm.jerryagenyi.xyz,http://localhost:3000 # Example: your LLM API hostname and a local dev server

# === CLOUDFLARE TUNNEL CONFIGURATION (Non-Sensitive - Token is sensitive) ===
# This specifies the name of the Cloudflare Tunnel managed in your Zero Trust Dashboard.
# This name should match the tunnel you create/rename in the Cloudflare dashboard.
TUNNEL_NAME=desktop-home-lab-tunnel


# === MODEL CONFIGURATION (Non-Sensitive - Primarily for Documentation) ===
# These variables document the primary model you intend to use with Ollama.
# NOTE: For the current hybrid setup (native Windows Ollama), these values
# are NOT directly consumed by the running Ollama server. The model is
# specified in your API requests (e.g., in N8N).
MODEL_NAME=qwen3:8b # Your primary model name (e.g., qwen3:8b, llama3:8b-instruct-q4_K_M)
MODEL_QUANTIZATION=q4_K_M # Quantisation level (e.g., q4_K_M)

# === OLLAMA PERFORMANCE TUNING (Non-Sensitive - Primarily for Documentation) ===
# These settings apply to the native Windows Ollama server if set as system environment variables,
# or when starting Ollama manually. They are documented here for reference.
OLLAMA_HOST=0.0.0.0:11434       # Ollama binds to this address (default is localhost)
OLLAMA_MAX_LOADED_MODELS=1      # Max models in memory
OLLAMA_NUM_PARALLEL=2           # Max parallel inference requests
OLLAMA_FLASH_ATTENTION=1        # Enable Flash Attention for performance (if supported by GPU)
OLLAMA_KEEP_ALIVE=24h           # Keep models loaded for this duration
OLLAMA_LOAD_TIMEOUT=300         # Timeout for model loading in seconds
MAX_REQUEST_SIZE=50MB           # Max size for API requests to Nginx
WORKER_PROCESSES=auto           # Nginx worker processes (auto = number of CPU cores)


# === AMD GPU-SPECIFIC OVERRIDES (Non-Sensitive - For Reference / Advanced Use) ===
# These are ROCm/GPU-specific environment variables for AMD GPUs.
# They are relevant if you were running Ollama directly in a container, or
# if you need to set them as system environment variables for native Ollama
# in specific troubleshooting scenarios.
HSA_OVERRIDE_GFX_VERSION=11.0.0 # For ROCm compatibility with specific AMD architectures
ROCM_VERSION=5.7                # Specify ROCm version if needed for containerised setups
GPU_MEMORY_FRACTION=0.9         # Fraction of GPU memory Ollama should use


# === CLOUDFLARE SECRETS & API KEYS (Sensitive) ===
# These values MUST be kept private and never exposed.
# Get your tunnel token from the Cloudflare Zero Trust Dashboard (Access > Tunnels > your tunnel > token).
CLOUDFLARE_TUNNEL_TOKEN=your_tunnel_token_here_from_cloudflare

# Your Cloudflare Account ID (found in Cloudflare dashboard overview).
CLOUDFLARE_ACCOUNT_ID=your_cloudflare_account_id_here

# API Key for your LLM service (optional, for custom authentication at Nginx layer or app).
# Generate a strong key, e.g., using: openssl rand -hex 32
API_KEY=your_secure_api_key_here

# Basic Authentication for Nginx (optional, if you want a simple password for access).
# Generate password hash using: htpasswd -n user_name (copy the full output, including 'user_name:')
BASIC_AUTH_USER=admin
BASIC_AUTH_PASS=admin:$apr1$... # Example: admin:$apr1$....... (copy full hash here)


# === VOLUME PATHS (Non-Sensitive - Adjust for your WSL2 Ubuntu username) ===
# These paths define where Docker volumes will map to your WSL2 Ubuntu filesystem.
# Change 'yourusername' to your actual Ubuntu username (run 'whoami' in WSL2 to find it).
MODELS_PATH=/home/yourusername/llama-server/models # For Ollama models (if containerised)
CONFIG_PATH=/home/yourusername/llama-server/config # For Nginx config, etc.
LOGS_PATH=/home/yourusername/llama-server/logs     # For Nginx and Docker container logs


# --- FINAL STEPS ---
# 1. Copy this content to a new file named `.env` in your project root.
# 2. Fill in all placeholder values (e.g., `your_tunnel_token_here`).
# 3. Ensure `MODELS_PATH`, `CONFIG_PATH`, `LOGS_PATH` reflect your WSL2 username.
# 4. Never commit your `.env` file to version control if it contains secrets!