# Example environment variables for Llama Local Server
# Copy to .env and fill in your values

# === SYSTEM CONFIGURATION ===
COMPOSE_PROJECT_NAME=llama-on-wsl2-server
DOCKER_BUILDKIT=1

# === MODEL CONFIGURATION ===
MODEL_NAME=llama3.2:11b-instruct-q4_K_M
MODEL_QUANTIZATION=q4_K_M
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_NUM_PARALLEL=2
OLLAMA_FLASH_ATTENTION=1

# === GPU CONFIGURATION ===
# AMD GPU (ROCm)
HSA_OVERRIDE_GFX_VERSION=11.0.0
ROCM_VERSION=5.7
GPU_MEMORY_FRACTION=0.9

# === NETWORKING ===
HOST_PORT=8080
INTERNAL_PORT=11434
NGINX_PORT=80
ALLOWED_ORIGINS=https://your-domain.com,chrome-extension://

# === CLOUDFLARE TUNNEL ===
# IMPORTANT: Get this token from Cloudflare Zero Trust Dashboard
# DO NOT let scripts auto-create tunnels - use manual token method
CLOUDFLARE_TUNNEL_TOKEN=your_tunnel_token_here_from_dashboard
TUNNEL_NAME=llama-api
CLOUDFLARE_ACCOUNT_ID=your_account_id

# === SECURITY ===
# Generate a strong API key: openssl rand -hex 32
API_KEY=your_secure_api_key_here
BASIC_AUTH_USER=admin
# Generate password hash: htpasswd -n admin
BASIC_AUTH_PASS=your_secure_password

# === PATHS (WSL2) ===
# Change 'yourusername' to your actual Ubuntu username
MODELS_PATH=/home/yourusername/llama-server/models
CONFIG_PATH=/home/yourusername/llama-server/config
LOGS_PATH=/home/yourusername/llama-server/logs

# === PERFORMANCE TUNING ===
OLLAMA_KEEP_ALIVE=24h
OLLAMA_LOAD_TIMEOUT=300
MAX_REQUEST_SIZE=50MB
WORKER_PROCESSES=auto



# .env for Llama Local Server (WSL2, Docker, Cloudflare Tunnel)
# Copy this file to .env and fill in the required values.

# === SYSTEM CONFIGURATION ===
COMPOSE_PROJECT_NAME=llama-on-wsl2-server
DOCKER_BUILDKIT=1

# === MODEL CONFIGURATION ===
MODEL_NAME=llama3.2:11b-instruct-q4_K_M
MODEL_QUANTIZATION=q4_K_M
OLLAMA_HOST=0.0.0.0:11434
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_NUM_PARALLEL=2
OLLAMA_FLASH_ATTENTION=1

# === GPU CONFIGURATION ===
# AMD GPU (ROCm)
HSA_OVERRIDE_GFX_VERSION=11.0.0
ROCM_VERSION=5.7
GPU_MEMORY_FRACTION=0.9

# === NETWORKING ===
HOST_PORT=8080
INTERNAL_PORT=11434
NGINX_PORT=80
# Set to your public domain (e.g., https://www.jerryagenyi.xyz) and any browser extension IDs you trust.
# Example: https://www.jerryagenyi.xyz,chrome-extension://abcdef1234567890
ALLOWED_ORIGINS=https://www.jerryagenyi.xyz

# === CLOUDFLARE TUNNEL ===
# Get this token from Cloudflare Zero Trust Dashboard (Access > Tunnels > your tunnel > token)
CLOUDFLARE_TUNNEL_TOKEN=eyJ...
# Find your account ID in the Cloudflare dashboard/profile or in the URL when managing your domain
CLOUDFLARE_ACCOUNT_ID=...

# === SECURITY ===
# Generate a strong API key: openssl rand -hex 32
API_KEY=...
BASIC_AUTH_USER=admin
# Generate password hash: htpasswd -n admin (copy the full output)
BASIC_AUTH_PASS=admin:$apr....

# === PATHS (WSL2) ===
# Change 'yourusername' to your actual Ubuntu username (run 'whoami' in WSL2 to find it)
MODELS_PATH=/home/jerryagenyi/llama-server/models
CONFIG_PATH=/home/jerryagenyi/llama-server/config
LOGS_PATH=/home/jerryagenyi/llama-server/logs

# === PERFORMANCE TUNING ===
OLLAMA_KEEP_ALIVE=24h
OLLAMA_LOAD_TIMEOUT=300
MAX_REQUEST_SIZE=50MB
WORKER_PROCESSES=auto

# ---
# Fill in all values marked <-- REQUIRED or UPDATE.
# Remove comments after you have replaced all placeholders.
# Keep this file private and never commit secrets to a public repository.
